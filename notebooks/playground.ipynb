{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 545,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "username = getpass.getuser()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "outputs": [],
   "source": [
    "fashion_train_filename = f'/Users/{username}/data/fashion_mnist/fashion-mnist_train.csv'\n",
    "fashion_val_filename = f'/Users/{username}/data/fashion_mnist/fashion-mnist_test.csv'\n",
    "\n",
    "text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
    "                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "outputs": [
    {
     "data": {
      "text/plain": "2"
     },
     "execution_count": 595,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])"
     },
     "execution_count": 597,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(10)[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "one_hot(): argument 'input' (position 1) must be Tensor, not numpy.int64",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [592]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mone_hot\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_classes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mTypeError\u001B[0m: one_hot(): argument 'input' (position 1) must be Tensor, not numpy.int64"
     ]
    }
   ],
   "source": [
    "F.one_hot(y, num_classes=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "idx = 0\n",
    "df = pd.read_csv(fashion_train_filename)\n",
    "label = df.iloc[idx][0]\n",
    "img = df.iloc[idx][1:]\n",
    "\n",
    "img = np.array(img).reshape((28, 28)).astype(np.uint8)\n",
    "print(img.shape)\n",
    "img = Image.fromarray(img)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "shape",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[0;32mIn [554]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mimg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/urim_base/lib/python3.8/site-packages/PIL/Image.py:517\u001B[0m, in \u001B[0;36mImage.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m    515\u001B[0m     deprecate(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mImage categories\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m10\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mis_animated\u001B[39m\u001B[38;5;124m\"\u001B[39m, plural\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_category\n\u001B[0;32m--> 517\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(name)\n",
      "\u001B[0;31mAttributeError\u001B[0m: shape"
     ]
    }
   ],
   "source": [
    "img.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "outputs": [
    {
     "data": {
      "text/plain": "<PIL.Image.Image image mode=L size=28x28>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACD0lEQVR4nLWSvW+NYRjGr/t+nvfrnJwXPa2PEh8VERHp0omkAwYJYWrCH8BCUonVwNSYhaUTxs6YsaiQSiXSEvTQNNrS056evs/78TzPbRASMfdar+uXXMMP2JQQAGgLAMMyE5d7Rh6/+G90cXK+JTeG78jyZxn7h8TgmxXdMUjHzuyOalFPMPjuT0mC93FX1SlXA8sdtqFvpgwSgAmCWzvW0nrbuGx6idc5C76170MAENjj51qRK6YiFhIXSF40D6Xr7MFgjJiuKshmibWVU9bWo+r7Q/jfh2YjY0QajbxIkkpb2w6U33q8pS2R9L1eh4nTuVfuxNukvTFwsH+1UTQnLwAsuEKVhKX6NPXFmJcrTdtRVNrV8w0QAV+LduilPjdfHl0sda9Pm87Bb5+4BsKxJ4s1F2lJJM/rSVEKKymLuDrC0LguUmgTZosJFV0bcuBCgq6qH1fvEZaXTCSiN6qozla8DU0cOSmpEffzUG9pnSIX1nwnVzqscVK6Toog6+7ikx/KEPCVVwllJi/Kwgd7n98d7JqeSzRxasFss5R7H1DlWbGu7Ja+dK5mpy7zzUcbQ8/GDxiQ4xqLsyYgWOyfHj27oGdHsa91W2URo7IBPODK4ClwGoBmjxZmKMkLzcREJBCbQQHiCSBdQT7GobXeC7yWfOeNB+z/aoLxwwtMjNB7J7ZsngMJyeYoDfwCgE0RBqCiu3sAAAAASUVORK5CYII=\n"
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class FashionDataset(Dataset):\n",
    "    def __init__(self, train_filename, img_size=(28, 28), num_classes=10,\n",
    "                 train_transforms=None):\n",
    "        self.df = pd.read_csv(train_filename)\n",
    "        self.img_size = img_size\n",
    "        self.num_classes = num_classes\n",
    "        self.train_transforms = train_transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label_idx = self.df.iloc[idx][0]\n",
    "        label = np.eye(self.num_classes)[label_idx]\n",
    "\n",
    "        img = self.df.iloc[idx][1:]\n",
    "        img = np.array(img).reshape(self.img_size).astype(np.uint8)\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        if self.train_transforms:\n",
    "            img = self.train_transforms(img)\n",
    "        return img, label"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "outputs": [],
   "source": [
    "from yullab.models import MLP\n",
    "from torchvision import transforms\n",
    "\n",
    "train_transforms = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "fashion_dataset = FashionDataset(fashion_train_filename,\n",
    "                                 train_transforms=train_transforms)\n",
    "dataloader = DataLoader(fashion_dataset, batch_size=32, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input must have 3 dimensions, got 5",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [586]\u001B[0m, in \u001B[0;36m<cell line: 14>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     17\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m#imgs = imgs.reshape(sequence_len, -1, input_size) #for rnn\u001B[39;00m\n\u001B[0;32m---> 21\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlstm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimgs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels)\n\u001B[1;32m     23\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m~/miniforge3/envs/urim_base/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Input \u001B[0;32mIn [527]\u001B[0m, in \u001B[0;36mLSTM.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     75\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m---> 76\u001B[0m     outputs, hidden \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlstm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     77\u001B[0m     \u001B[38;5;28mprint\u001B[39m(outputs\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m     78\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc1(outputs[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n",
      "File \u001B[0;32m~/miniforge3/envs/urim_base/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/miniforge3/envs/urim_base/lib/python3.8/site-packages/torch/nn/modules/rnn.py:767\u001B[0m, in \u001B[0;36mLSTM.forward\u001B[0;34m(self, input, hx)\u001B[0m\n\u001B[1;32m    763\u001B[0m     \u001B[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001B[39;00m\n\u001B[1;32m    764\u001B[0m     \u001B[38;5;66;03m# the user believes he/she is passing in.\u001B[39;00m\n\u001B[1;32m    765\u001B[0m     hx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpermute_hidden(hx, sorted_indices)\n\u001B[0;32m--> 767\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcheck_forward_args\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_sizes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    768\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m batch_sizes \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    769\u001B[0m     result \u001B[38;5;241m=\u001B[39m _VF\u001B[38;5;241m.\u001B[39mlstm(\u001B[38;5;28minput\u001B[39m, hx, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flat_weights, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_layers,\n\u001B[1;32m    770\u001B[0m                       \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbidirectional, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_first)\n",
      "File \u001B[0;32m~/miniforge3/envs/urim_base/lib/python3.8/site-packages/torch/nn/modules/rnn.py:692\u001B[0m, in \u001B[0;36mLSTM.check_forward_args\u001B[0;34m(self, input, hidden, batch_sizes)\u001B[0m\n\u001B[1;32m    687\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcheck_forward_args\u001B[39m(\u001B[38;5;28mself\u001B[39m,  \u001B[38;5;66;03m# type: ignore[override]\u001B[39;00m\n\u001B[1;32m    688\u001B[0m                        \u001B[38;5;28minput\u001B[39m: Tensor,\n\u001B[1;32m    689\u001B[0m                        hidden: Tuple[Tensor, Tensor],\n\u001B[1;32m    690\u001B[0m                        batch_sizes: Optional[Tensor],\n\u001B[1;32m    691\u001B[0m                        ):\n\u001B[0;32m--> 692\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcheck_input\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_sizes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    693\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_hidden_size(hidden[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_expected_hidden_size(\u001B[38;5;28minput\u001B[39m, batch_sizes),\n\u001B[1;32m    694\u001B[0m                            \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mExpected hidden[0] size \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, got \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    695\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_hidden_size(hidden[\u001B[38;5;241m1\u001B[39m], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_expected_cell_size(\u001B[38;5;28minput\u001B[39m, batch_sizes),\n\u001B[1;32m    696\u001B[0m                            \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mExpected hidden[1] size \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, got \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/miniforge3/envs/urim_base/lib/python3.8/site-packages/torch/nn/modules/rnn.py:201\u001B[0m, in \u001B[0;36mRNNBase.check_input\u001B[0;34m(self, input, batch_sizes)\u001B[0m\n\u001B[1;32m    199\u001B[0m expected_input_dim \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m batch_sizes \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m3\u001B[39m\n\u001B[1;32m    200\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;241m!=\u001B[39m expected_input_dim:\n\u001B[0;32m--> 201\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    202\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput must have \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m dimensions, got \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m    203\u001B[0m             expected_input_dim, \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mdim()))\n\u001B[1;32m    204\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_size \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m    205\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    206\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput.size(-1) must be equal to input_size. Expected \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, got \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m    207\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_size, \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)))\n",
      "\u001B[0;31mRuntimeError\u001B[0m: input must have 3 dimensions, got 5"
     ]
    }
   ],
   "source": [
    "from yullab.models import MLP\n",
    "\n",
    "lr = 0.001\n",
    "n_epochs = 30\n",
    "sequence_len = 28\n",
    "input_size = 28\n",
    "\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=lr)\n",
    "#criterion = nn.MultiLabelSoftMarginLoss() ## 공부할 것\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "total_batch = len(dataloader)\n",
    "train_losses = []\n",
    "for epoch in range(n_epochs):\n",
    "    total_cost = 0\n",
    "    for imgs, labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #imgs = imgs.reshape(sequence_len, -1, input_size) #for rnn\n",
    "\n",
    "        outputs = lstm(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_cost += loss.item()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    avg_loss = total_cost / len(dataloader)\n",
    "    print(f'Epoch: {epoch+1} Cost: {avg_loss}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "outputs": [
    {
     "data": {
      "text/plain": "2"
     },
     "execution_count": 591,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = dataset[0]\n",
    "y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30..  Training Loss: 1.574..  Test Loss: 1.546..  Test Accuracy: 0.460\n",
      "Epoch: 2/30..  Training Loss: 1.216..  Test Loss: 1.086..  Test Accuracy: 0.671\n",
      "Epoch: 3/30..  Training Loss: 1.080..  Test Loss: 1.070..  Test Accuracy: 0.667\n",
      "Epoch: 4/30..  Training Loss: 1.060..  Test Loss: 1.076..  Test Accuracy: 0.676\n",
      "Epoch: 5/30..  Training Loss: 1.048..  Test Loss: 1.090..  Test Accuracy: 0.672\n",
      "Epoch: 6/30..  Training Loss: 1.041..  Test Loss: 1.031..  Test Accuracy: 0.683\n",
      "Epoch: 7/30..  Training Loss: 0.853..  Test Loss: 0.846..  Test Accuracy: 0.771\n",
      "Epoch: 8/30..  Training Loss: 0.805..  Test Loss: 0.832..  Test Accuracy: 0.775\n",
      "Epoch: 9/30..  Training Loss: 0.802..  Test Loss: 0.820..  Test Accuracy: 0.768\n",
      "Epoch: 10/30..  Training Loss: 0.796..  Test Loss: 0.853..  Test Accuracy: 0.778\n",
      "Epoch: 11/30..  Training Loss: 0.792..  Test Loss: 0.839..  Test Accuracy: 0.763\n",
      "Epoch: 12/30..  Training Loss: 0.782..  Test Loss: 0.823..  Test Accuracy: 0.771\n",
      "Epoch: 13/30..  Training Loss: 0.784..  Test Loss: 0.831..  Test Accuracy: 0.775\n",
      "Epoch: 14/30..  Training Loss: 0.779..  Test Loss: 0.818..  Test Accuracy: 0.777\n",
      "Epoch: 15/30..  Training Loss: 0.775..  Test Loss: 0.825..  Test Accuracy: 0.778\n",
      "Epoch: 16/30..  Training Loss: 0.777..  Test Loss: 0.817..  Test Accuracy: 0.780\n",
      "Epoch: 17/30..  Training Loss: 0.771..  Test Loss: 0.814..  Test Accuracy: 0.775\n"
     ]
    }
   ],
   "source": [
    "# for cnn, mlp\n",
    "train_transforms = transforms.Compose([transforms.ToTensor(),\n",
    "                                       transforms.Normalize(0, 1)])\n",
    "\n",
    "train_dataset = FashionDataset(fashion_train_filename,\n",
    "                         train_transforms=train_transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataset = FashionDataset(fashion_val_filename,\n",
    "                             train_transforms=train_transforms)\n",
    "val_loader = DataLoader(val_dataset, batch_size=10000, shuffle=False)\n",
    "net = MLP()\n",
    "\n",
    "#for cnn, mlp\n",
    "lr = 0.005\n",
    "n_epochs = 30\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "total_batch = len(train_loader)\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for epoch in range(n_epochs):\n",
    "    running_loss, train_losses = train(train_loader, net, criterion, optimizer)\n",
    "    test_loss, test_losses, accuracy  = test(val_loader, net, criterion)\n",
    "    #\n",
    "    #\n",
    "    # running_loss = 0\n",
    "    # for imgs, labels in train_loader:\n",
    "    #     optimizer.zero_grad()\n",
    "    #\n",
    "    #     imgs = imgs.reshape(-1, 784)\n",
    "    #     outputs = net(imgs)\n",
    "    #     loss = criterion(outputs, labels)\n",
    "    #     loss.backward()\n",
    "    #\n",
    "    #     optimizer.step()\n",
    "    #\n",
    "    #     running_loss += loss.item()\n",
    "    #     train_losses.append(loss.item())\n",
    "    #\n",
    "    # test_loss = 0\n",
    "    # accuracy = 0\n",
    "    # with torch.no_grad():\n",
    "    #     for images, labels in val_loader:\n",
    "    #         images = images.reshape(-1, 784)\n",
    "    #         outputs = net(images)\n",
    "    #\n",
    "    #         test_loss += criterion(outputs, labels)\n",
    "    #\n",
    "    #         top_p, top_class = outputs.topk(1, dim=1)\n",
    "    #         _, label_idx = labels.topk(1, dim=1)\n",
    "    #\n",
    "    #         equals = top_class == label_idx\n",
    "    #         accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "    #\n",
    "    # train_losses.append(running_loss/len(train_loader))\n",
    "    # test_losses.append(test_loss/len(val_loader))\n",
    "\n",
    "    print(\"Epoch: {}/{}.. \".format(epoch+1, n_epochs),\n",
    "          \"Training Loss: {:.3f}.. \".format(running_loss/len(train_loader)),\n",
    "          \"Test Loss: {:.3f}.. \".format(test_loss/len(val_loader)),\n",
    "          \"Test Accuracy: {:.3f}\".format(accuracy/len(val_loader)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}